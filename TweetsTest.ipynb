{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting read_csv\n",
      "done with read_csv, starting labelencoder\n",
      "done with label encoder\n",
      "starting loop\n",
      "done with loop, starting vectorizer\n",
      "done with vectorizer\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Claire Roush\n",
    "#CS 167 - Prof Manley\n",
    "#December 12, 2016\n",
    "#Airline Tweets text data classification with SVM, PCA, Naive Bayes, and Random Forests.\n",
    "\n",
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import numpy\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection as cv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#stopSet = set(stopwords.words(\"english\"))\n",
    "cleaned_data = []\n",
    "print(\"starting read_csv\")\n",
    "data=pandas.read_csv(\"Tweets.csv\", usecols=[1,2,10])\n",
    "print(\"done with read_csv, starting labelencoder\")\n",
    "\n",
    "le_sentiment = preprocessing.LabelEncoder()\n",
    "\n",
    "#to convert into numbers\n",
    "# source: https://www.kaggle.com/c/titanic/forums/t/5379/handling-categorical-data-with-sklearn?forumMessageId=139270\n",
    "\n",
    "data.airline_sentiment = le_sentiment.fit_transform(data.airline_sentiment)\n",
    "print(\"done with label encoder\")\n",
    "\n",
    "#df_sentiment = pandas.get_dummies(data[\"airline_sentiment\"])\n",
    "#data = pandas.concat([data, df_sentiment], axis=1)\n",
    "#data = data.drop(\"airline_sentiment\",1)\n",
    "#columns to use for sentiment are now 'neutral', 'positive' and 'negative\n",
    "\n",
    "#print(data[\"text\"][0])\n",
    "#cited source: https://pythonprogramming.net/stop-words-nltk-tutorial/\n",
    "stopSet = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_it(wordData):\n",
    "    minus_punctuation = re.sub(\"[^a-zA-Z]\",\" \",wordData)\n",
    "    to_lowercase = minus_punctuation.lower()\n",
    "    tokenized = to_lowercase.split()\n",
    "    #filtered = [i for i in tokenized if not i in stopSet]\n",
    "    return tokenized\n",
    "\n",
    "print(\"starting loop\")\n",
    "i = 0\n",
    "while (i<len(data)):\n",
    "    temp = clean_it(data[\"text\"][i])\n",
    "    temp = ' '.join(temp)\n",
    "    cleaned_data.append(temp)\n",
    "    #print(temp)\n",
    "    i=i+1\n",
    "print(\"done with loop, starting vectorizer\")\n",
    "#print(cleaned_data[0])\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "word_columns = vectorizer.fit_transform(cleaned_data)\n",
    "#print(word_columns)\n",
    "#convert to numpy array so we can feed it #into learning algorithm\n",
    "word_columns = word_columns.toarray()\n",
    "print(\"done with vectorizer\")\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "(train_data, test_data, train_target, test_target) = cv.train_test_split(word_columns, data[\"airline_sentiment\"], test_size = 0.2)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment                                                 1\n",
       "airline_sentiment_confidence                                      1\n",
       "text                            @VirginAmerica What @dhepburn said.\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746926229508\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_data,train_target)\n",
    "preds = mnb.predict(test_data)\n",
    "print(accuracy_score(preds,test_target))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501024590164\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_data,train_target)\n",
    "preds_2 = gnb.predict(test_data)\n",
    "print(accuracy_score(preds_2,test_target))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA #new one!\n",
    "pca = PCA(n_components=400,whiten=True)\n",
    "pca.fit(train_data)\n",
    "transformed_train_data = pca.transform(train_data) \n",
    "transformed_test_data = pca.transform(test_data)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77356557377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.5, 'fit_prior': False}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "params = {'alpha': (0,0.5,1,2,4),\n",
    "          'fit_prior': (True, False),\n",
    "         }\n",
    "mnb = grid_search.GridSearchCV(MultinomialNB(),params)\n",
    "mnb.fit(train_data,train_target)\n",
    "preds_6 = mnb.predict(test_data)\n",
    "print(accuracy_score(preds_6,test_target))\n",
    "mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.758879781421\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=200, criterion='entropy', max_features='log2')\n",
    "rf.fit(train_data,train_target)\n",
    "rf_predictions = rf.predict(test_data)\n",
    "print(accuracy_score(rf_predictions,test_target))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc = SVC(gamma=0.1)\n",
    "svc.fit(train_data,train_target)\n",
    "preds_3 = svc.predict(test_data)\n",
    "print(accuracy_score(preds_3,test_target))\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(gamma=0.1)\n",
    "svc.fit(transformed_train_data,train_target)\n",
    "preds_3 = svc.predict(transformed_test_data)\n",
    "print(accuracy_score(preds_3,test_target))\n",
    "print(\"done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import grid_search\n",
    "params = {'C': (0.1,1,2,3),\n",
    "          'kernel': ('linear','poly','rbf'),\n",
    "          'degree': (0,1,2,3),\n",
    "          'gamma': (0.1,1),\n",
    "          'coef0': (0.1,1,2)\n",
    "         }\n",
    "svc = grid_search.GridSearchCV(SVC(),params)\n",
    "svc.fit(transformed_train_data,train_target)\n",
    "preds_6 = svc.predict(transformed_test_data)\n",
    "print(accuracy_score(preds_6,test_target))\n",
    "svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1774a7a90>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEKCAYAAADn+anLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmaZHBJmZpbJIWFmZpkcEmZmlun/Aym64lBVFBFWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1176848d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pca = [20,  40,  60,  80,  100, 120, 140, 160,  180,  200,  400, 600, 800]\n",
    "svc = [69.8,73.4,74.5,75.3,76.6,77.4,77.7,77.8, 77.7, 78.0, 78.7, 80,  76.1]\n",
    "\n",
    "plt.ylabel('SVC Accuracy')\n",
    "plt.xlabel('PCA Components')\n",
    "plt.scatter(svc,pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text Analysis of Twitter Airline Data ##\n",
    "\n",
    "#### The Data ####\n",
    "\n",
    "I retrieved the dataset from: https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "\n",
    "The dataset is comprised of 14,641 tweets regarding airlines Virgin America, United, Southwest, Delta, US Airways, and American. The task cited by the Kaggle page is to determine sentiment and to group sentiments to each airline. The end results would be a complete picture of negative, neutral, and positive sentiments on Twitter for each given airline. I'm going to simplify the task a bit to only predict sentiment. \n",
    "\n",
    "#### Clean Up ####\n",
    "\n",
    "The first thing I did while examining the data in Excel was to decide which columns I wanted to use. I decided on ```airline_sentiment```, ```airline_sentiment_confidence```, and ```text```, which are the sentiment classification, a confidence rating (weight), and the tweet body, respectively. I read in only these columns using ```usecols```. The other columns were ignored because their data didn't seem important to this project. I intended to use the weights (confidence rating), but this proved to be a bit more difficult than I originally thought. \n",
    "\n",
    "\n",
    "For classification algorithm purposes, I needed to convert the ```positive, neutral, negative``` classifications in the ```airline_sentiment``` column to integers. I did this with ```LabelEncoder```. I was then left with data that looked like:\n",
    "\n",
    "<img src=\"data.png\" width=50%, height=50%>\n",
    "\n",
    "I then removed punctuation, converted to lowercase and tokenized in my ```clean_it``` function, which was called by a loop and performed on each tweet before joining back into a string and appended to ```cleaned_data```.\n",
    "\n",
    "I used ```CountVectorizer``` to convert my appended strings in ```cleaned_data``` to a word count vector. I then converted to a numpy array and split into a training and testing set. It is now ready to test with different algorithms.. \n",
    "\n",
    "#### Naive Bayes ####\n",
    "\n",
    "The first Naive Bayes algorithm I used was Multinomial Naive Bayes. The accuracy straight \"out of the box\" for this was ```0.7634```. The second Naive Bayes algorithm I used was Gaussian Naive Bayes. The accuracy score straight out of the box for GNB was ```0.492```. I decided to do further testing with Multinomial Naive Bayes. \n",
    "\n",
    "At first I chose not to perform a Grid Search with Naive Bayes algorithms, as the algorithms are simple enough to test parameters manually. Later, though, I did use Grid Search to find the best ```alpha``` value of 0.5, as well as a ```prior_search``` of ```False```. The accuracy of MNB with ```alpha=0.5,prior_search=False``` was 77.4. The accuracy was a bit better, but still not as good as I expected. I felt that I had run out of options with MNB. With the removal of stopwords, the accuracy of Multinomial Naive Bayes fell an average of ```0.02```. \n",
    "\n",
    "I tried Gaussian Naive Bayes next with transformed data from Principal Component Analysis. With ```n_components``` from 50-800, Gaussian Naive Bayes performed at ```0.618``` accuracy at its best, with ```n_components=50```. With the removal of stopwords, Gaussian Naive Bayes accuracy dropped an overage of ```0.03```. \n",
    "\n",
    "#### Principal Component Analysis and Support Vector Machines ####\n",
    "\n",
    "I ran a series of tests to determine the best number of components for PCA according to SVC.\n",
    "\n",
    "<img src=\"PCASVC.png\" width=50%, height=50%>\n",
    "\n",
    "I found that right around ```n_components=600``` was a good area for SVC accuracy. \n",
    "\n",
    "Next, I ran a grid search for Support Vector Machines with the following parameters:\n",
    "\n",
    "```params = {'C': (0.000000001,0.000001,0.00001,0.001,0.1,0,1,2,3),\n",
    "          'kernel': ('linear','poly','rbf','sigmoid','precomputed'),\n",
    "          'degree': (0,1,2,3,4,5,6),\n",
    "          'gamma': (0.00001,0.0001,0.001,0.1,1,10),\n",
    "          'coef0': (0.0001,0.001,0.1,1,2,3,4),\n",
    "          'shrinking': (True,False),\n",
    "          'tol': (0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.1,1)\n",
    "         }\n",
    "```\n",
    "\n",
    "This grid search ran over 12 hours and still wasn't complete. As a result, I interrupted the kernel, manually tested different parameters and found ```gamma=0.1, C=1, kernel='rbf',degree=4,coef0=0.0``` to produce the best average accuracy of ```0.823```. \n",
    "\n",
    "\n",
    "#### Random Forest ####\n",
    "\n",
    "Out of curiosity, and after reading <b><a href=\"https://pdfs.semanticscholar.org/9b2f/84d85e5b6979bf375a2d4b15f7526597fc70.pdf\">An Improved Random Forest Classifier for Text Categorization</a></b>, I thought I'd try Random Forest on this dataset. The paper proposes to improve Random Forest algorithm by enhancing the performance of individual trees by \"a feature weighted method proposed by Amaratunga\" (1), and by watching for \"bad\" trees and cutting them out before their input distorts the ultimate prediction. This was pretty interesting to me, and made me curious as to the performance of the ScikitLearn Random Forest Classifier without the optimizations proposed in the paper.\n",
    "\n",
    "The following table shows the accuracy change dependent upon ```n_estimators```. \n",
    "\n",
    "\n",
    "| n_estimators| Accuracy   \n",
    "| ------------|:-----------:|\n",
    "| 10          | 0.752       |\n",
    "| 100         | 0.765       |\n",
    "| 200         | 0.783       |\n",
    "| 300         | 0.763       |\n",
    "\n",
    "I tried to recreate the optimizations proposed in the paper, but wasn't entirely sure how to do so. With \n",
    "*max_features='log2',min_samples_split=4,criterion='entropy',n_estimators=200*, a \"bad\" tree is perhaps less likely, though I wasn't positive how to implement the feature weighted optimization. The accuracy of this algorithm with these paramaters was ```0.76```. \n",
    "\n",
    "#### Analyzation ####\n",
    "\n",
    "\n",
    "Over all, Support Vector Machines performed best with the transformed data from Principal Component Analysis. I'm not sure why this is the case. Processing the data into word count vectors, then projecting them to a higher dimension with PCA may have made the data separable by SVM. Perhaps a decent linear separation exists in this case.\n",
    "\n",
    "Naive Bayes' performance surprised me. It makes sense to me that Multinomial Naive Bayes performed what it did, when word freqency would be used in maximum likelihood estimation. Word frequency is probably a really important part of this data, so it follows that Multinomial Naive Bayes outperformed Gaussian. However, I still expected the Naive Bayes algorithms to perform better than they did.\n",
    "\n",
    "The removal of stopwords, overall, seemed to hurt performance in all algorithms. This was interesting.\n",
    "\n",
    "Random Forest Classifier's performance was about where I expected. The article linked above proposed interesting optimizations which I tried to recreate, partially, though parameters. However, this produced accuracy scores less than impressive. Of course, the paper's optimization proposals are more than the scope of this project. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
